{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e640f8e2-e71e-4a19-a537-99c516524ddc",
   "metadata": {},
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3762d02-2a31-4ff3-9e7d-7da20ba4cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe.minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file = \"./output/tokenizer/my_tokenizer.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e84980-f22e-44e4-ada8-284c30083bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b'a ',\n",
       " 258: b'. ',\n",
       " 259: b'i ',\n",
       " 260: b'ha',\n",
       " 261: b't ',\n",
       " 262: b'in',\n",
       " 263: b'o ',\n",
       " 264: b'r ',\n",
       " 265: b's ',\n",
       " 266: b'd ',\n",
       " 267: b'l ',\n",
       " 268: b'th',\n",
       " 269: b'an',\n",
       " 270: b'on',\n",
       " 271: b'y ',\n",
       " 272: b'er',\n",
       " 273: b'k ',\n",
       " 274: b'en',\n",
       " 275: b'h ',\n",
       " 276: b'aa',\n",
       " 277: b'ar',\n",
       " 278: b'ou',\n",
       " 279: b're',\n",
       " 280: b'at',\n",
       " 281: b'll ',\n",
       " 282: b'g ',\n",
       " 283: b'to ',\n",
       " 284: b'hai ',\n",
       " 285: b'ya ',\n",
       " 286: b'? ',\n",
       " 287: b'in ',\n",
       " 288: b'm ',\n",
       " 289: b'ing ',\n",
       " 290: b'or',\n",
       " 291: b'hi ',\n",
       " 292: b'es',\n",
       " 293: b'ma',\n",
       " 294: b'.. ',\n",
       " 295: b'is',\n",
       " 296: b'le',\n",
       " 297: b'de',\n",
       " 298: b'al',\n",
       " 299: b'hi',\n",
       " 300: b'on ',\n",
       " 301: b', ',\n",
       " 302: b'he',\n",
       " 303: b'me ',\n",
       " 304: b'it',\n",
       " 305: b'ro',\n",
       " 306: b'ra',\n",
       " 307: b'se ',\n",
       " 308: b'ga ',\n",
       " 309: b'ho',\n",
       " 310: b'ka ',\n",
       " 311: b'ga',\n",
       " 312: b'hai',\n",
       " 313: b'or ',\n",
       " 314: b'ha ',\n",
       " 315: b'le ',\n",
       " 316: b'the ',\n",
       " 317: b'Ha',\n",
       " 318: b'er ',\n",
       " 319: b'u ',\n",
       " 320: b'... ',\n",
       " 321: b'lo',\n",
       " 322: b'ti',\n",
       " 323: b'la',\n",
       " 324: b'you',\n",
       " 325: b'li',\n",
       " 326: b'and ',\n",
       " 327: b'ed ',\n",
       " 328: b'! ',\n",
       " 329: b'ta ',\n",
       " 330: b'un',\n",
       " 331: b'I ',\n",
       " 332: b'us',\n",
       " 333: b'b ',\n",
       " 334: b'f ',\n",
       " 335: b'ac',\n",
       " 336: b'is ',\n",
       " 337: b'nu',\n",
       " 338: b've ',\n",
       " 339: b'ka',\n",
       " 340: b'to',\n",
       " 341: b'di',\n",
       " 342: b'sa',\n",
       " 343: b'ko ',\n",
       " 344: b'pa',\n",
       " 345: b'na ',\n",
       " 346: b'wi',\n",
       " 347: b'p ',\n",
       " 348: b'me',\n",
       " 349: b'\\n\\n',\n",
       " 350: b'an ',\n",
       " 351: b'co',\n",
       " 352: b'you ',\n",
       " 353: b'null ',\n",
       " 354: b'et',\n",
       " 355: b'de ',\n",
       " 356: b'bo',\n",
       " 357: b'th ',\n",
       " 358: b'ic',\n",
       " 359: b'st',\n",
       " 360: b'for ',\n",
       " 361: b'ec',\n",
       " 362: b'\\xf0\\x9f',\n",
       " 363: b'ke ',\n",
       " 364: b'ye ',\n",
       " 365: b'bhi ',\n",
       " 366: b'w ',\n",
       " 367: b'al ',\n",
       " 368: b'wa',\n",
       " 369: b'ur',\n",
       " 370: b'ge ',\n",
       " 371: b'hai. ',\n",
       " 372: b'ba',\n",
       " 373: b'kr ',\n",
       " 374: b'ere ',\n",
       " 375: b'et ',\n",
       " 376: b'it ',\n",
       " 377: b'ch ',\n",
       " 378: b'cha',\n",
       " 379: b'te ',\n",
       " 380: b'da',\n",
       " 381: b'no',\n",
       " 382: b'ye',\n",
       " 383: b'he ',\n",
       " 384: b'hu',\n",
       " 385: b'ap',\n",
       " 386: b'tha',\n",
       " 387: b'ni ',\n",
       " 388: b'0 ',\n",
       " 389: b'es ',\n",
       " 390: b'na',\n",
       " 391: b'ati',\n",
       " 392: b'kr',\n",
       " 393: b'Hao ',\n",
       " 394: b'ly ',\n",
       " 395: b'ex',\n",
       " 396: b'. A',\n",
       " 397: b'main ',\n",
       " 398: b'mp',\n",
       " 399: b'ri',\n",
       " 400: b'ent ',\n",
       " 401: b'ter',\n",
       " 402: b'. T',\n",
       " 403: b'ki ',\n",
       " 404: b'en ',\n",
       " 405: b'pro',\n",
       " 406: b'se',\n",
       " 407: b'ta',\n",
       " 408: b'are ',\n",
       " 409: b'um',\n",
       " 410: b'of ',\n",
       " 411: b'n ',\n",
       " 412: b'kya ',\n",
       " 413: b'all ',\n",
       " 414: b'be ',\n",
       " 415: b'ch',\n",
       " 416: b'ik',\n",
       " 417: b'ya',\n",
       " 418: b'ata ',\n",
       " 419: b'ja',\n",
       " 420: b'ho ',\n",
       " 421: b'si',\n",
       " 422: b'hu ',\n",
       " 423: b'gh',\n",
       " 424: b'mo',\n",
       " 425: b'ne ',\n",
       " 426: b'ad',\n",
       " 427: b'ra ',\n",
       " 428: b'ing',\n",
       " 429: b'as',\n",
       " 430: b'mi',\n",
       " 431: b'do',\n",
       " 432: b'ff',\n",
       " 433: b'fi',\n",
       " 434: b'tha ',\n",
       " 435: b'j ',\n",
       " 436: b'kar',\n",
       " 437: b're ',\n",
       " 438: b'la ',\n",
       " 439: b'with ',\n",
       " 440: b'wor',\n",
       " 441: b'ce ',\n",
       " 442: b'te',\n",
       " 443: b'baa',\n",
       " 444: b'con',\n",
       " 445: b'Thi',\n",
       " 446: b'we',\n",
       " 447: b'so',\n",
       " 448: b'raha ',\n",
       " 449: b'am ',\n",
       " 450: b'ge',\n",
       " 451: b'ate ',\n",
       " 452: b'have ',\n",
       " 453: b'sha',\n",
       " 454: b'at ',\n",
       " 455: b'go',\n",
       " 456: b'da ',\n",
       " 457: b'An',\n",
       " 458: b'h. ',\n",
       " 459: b'ld ',\n",
       " 460: b'..',\n",
       " 461: b'null null ',\n",
       " 462: b'will ',\n",
       " 463: b'su',\n",
       " 464: b'waa',\n",
       " 465: b'.\\n',\n",
       " 466: b'ation ',\n",
       " 467: b'.\\n\\n',\n",
       " 468: b'Haa ',\n",
       " 469: b'ey ',\n",
       " 470: b'op',\n",
       " 471: b'st ',\n",
       " 472: b'jaa',\n",
       " 473: b'can ',\n",
       " 474: b'Ma',\n",
       " 475: b'dek',\n",
       " 476: b'day ',\n",
       " 477: b'ki',\n",
       " 478: b'5 ',\n",
       " 479: b'po',\n",
       " 480: b'ab',\n",
       " 481: b'ent',\n",
       " 482: b'\\xf0\\x9f\\x98',\n",
       " 483: b'ce',\n",
       " 484: b'uch ',\n",
       " 485: b'len',\n",
       " 486: b'one ',\n",
       " 487: b'as ',\n",
       " 488: b'vi',\n",
       " 489: b'yega ',\n",
       " 490: b'call ',\n",
       " 491: b'. O',\n",
       " 492: b'ne',\n",
       " 493: b'2 ',\n",
       " 494: b'1 ',\n",
       " 495: b'we ',\n",
       " 496: b'ag',\n",
       " 497: b'ed',\n",
       " 498: b'\\xe2\\x80',\n",
       " 499: b'that ',\n",
       " 500: b'ana ',\n",
       " 501: b'Lo',\n",
       " 502: b'ss ',\n",
       " 503: b'Ac',\n",
       " 504: b'per',\n",
       " 505: b'cha ',\n",
       " 506: b'ice ',\n",
       " 507: b'khe',\n",
       " 508: b'mor',\n",
       " 509: b'Thik ',\n",
       " 510: b'bu',\n",
       " 511: b'. W',\n",
       " 512: b'll',\n",
       " 513: b'this ',\n",
       " 514: b'ek ',\n",
       " 515: b'qu',\n",
       " 516: b'ise ',\n",
       " 517: b': ',\n",
       " 518: b'lin',\n",
       " 519: b'pu',\n",
       " 520: b'ver',\n",
       " 521: b'? Y',\n",
       " 522: b'rom ',\n",
       " 523: b'kar ',\n",
       " 524: b'oun',\n",
       " 525: b'den',\n",
       " 526: b'ck ',\n",
       " 527: b'mb',\n",
       " 528: b'off',\n",
       " 529: b'bh',\n",
       " 530: b'che',\n",
       " 531: b'mes',\n",
       " 532: b'ould ',\n",
       " 533: b'for',\n",
       " 534: b'!!',\n",
       " 535: b'my ',\n",
       " 536: b'iket',\n",
       " 537: b'pe',\n",
       " 538: b'ment ',\n",
       " 539: b'from ',\n",
       " 540: b'was ',\n",
       " 541: b'. S',\n",
       " 542: b'unga ',\n",
       " 543: b'lea',\n",
       " 544: b'han',\n",
       " 545: b'ill ',\n",
       " 546: b'any ',\n",
       " 547: b've',\n",
       " 548: b'pe ',\n",
       " 549: b'tion ',\n",
       " 550: b'00',\n",
       " 551: b'De',\n",
       " 552: b'liye ',\n",
       " 553: b'king ',\n",
       " 554: b'am',\n",
       " 555: b'Ye ',\n",
       " 556: b'era ',\n",
       " 557: b'ght ',\n",
       " 558: b'- ',\n",
       " 559: b'comp',\n",
       " 560: b'uj',\n",
       " 561: b'use ',\n",
       " 562: b\"'s \",\n",
       " 563: b'ru',\n",
       " 564: b'hoga ',\n",
       " 565: b'\\xe0\\xa4',\n",
       " 566: b'diya ',\n",
       " 567: b'ga. ',\n",
       " 568: b'inter',\n",
       " 569: b'You',\n",
       " 570: b'!\\n',\n",
       " 571: b'ted ',\n",
       " 572: b'not ',\n",
       " 573: b'? N',\n",
       " 574: b'ard ',\n",
       " 575: b'Hao',\n",
       " 576: b'oo',\n",
       " 577: b'il',\n",
       " 578: b'up',\n",
       " 579: b'be',\n",
       " 580: b'il ',\n",
       " 581: b'ta hai ',\n",
       " 582: b'Us',\n",
       " 583: b'\\xe2\\x80\\x99',\n",
       " 584: b'acc',\n",
       " 585: b'tu ',\n",
       " 586: b'gaya ',\n",
       " 587: b'gar ',\n",
       " 588: b'nai ',\n",
       " 589: b'ks ',\n",
       " 590: b'gi',\n",
       " 591: b'est ',\n",
       " 592: b'get ',\n",
       " 593: b'out ',\n",
       " 594: b'ust ',\n",
       " 595: b'\\xf0\\x9f\\x98\\x82',\n",
       " 596: b'ar ',\n",
       " 597: b'kiya ',\n",
       " 598: b'time ',\n",
       " 599: b'tr',\n",
       " 600: b'ta hu ',\n",
       " 601: b'so ',\n",
       " 602: b'kuch ',\n",
       " 603: b'pl',\n",
       " 604: b'ja ',\n",
       " 605: b'. H',\n",
       " 606: b'go ',\n",
       " 607: b'tra',\n",
       " 608: b'!!! ',\n",
       " 609: b'3 ',\n",
       " 610: b'ation',\n",
       " 611: b'bhai ',\n",
       " 612: b'your ',\n",
       " 613: b'. K',\n",
       " 614: b'ol',\n",
       " 615: b\"I'\",\n",
       " 616: b'ess ',\n",
       " 617: b'ree ',\n",
       " 618: b'ad ',\n",
       " 619: b'wo ',\n",
       " 620: b'day',\n",
       " 621: b'te h ',\n",
       " 622: b'jo',\n",
       " 623: b'min',\n",
       " 624: b'log ',\n",
       " 625: b'ur ',\n",
       " 626: b'gar',\n",
       " 627: b'Or ',\n",
       " 628: b'Tu ',\n",
       " 629: b'isa ',\n",
       " 630: b'sit',\n",
       " 631: b'wh',\n",
       " 632: b'Sum',\n",
       " 633: b'aaya ',\n",
       " 634: b'bac',\n",
       " 635: b'now ',\n",
       " 636: b'aa ',\n",
       " 637: b'if ',\n",
       " 638: b'man',\n",
       " 639: b'ts ',\n",
       " 640: b'ther ',\n",
       " 641: b'Bhai ',\n",
       " 642: b'ect ',\n",
       " 643: b'do ',\n",
       " 644: b'ko',\n",
       " 645: b'Sa',\n",
       " 646: b'par',\n",
       " 647: b'res',\n",
       " 648: b'dekh ',\n",
       " 649: b'abhi ',\n",
       " 650: b'Accha ',\n",
       " 651: b'lega ',\n",
       " 652: b'mai ',\n",
       " 653: b'In',\n",
       " 654: b'. P',\n",
       " 655: b'kin ',\n",
       " 656: b'di ',\n",
       " 657: b'hai... ',\n",
       " 658: b'kh',\n",
       " 659: b'ure ',\n",
       " 660: b\"'t \",\n",
       " 661: b'ul',\n",
       " 662: b'tion',\n",
       " 663: b'. M',\n",
       " 664: b'ujhe ',\n",
       " 665: b'? A',\n",
       " 666: b'app',\n",
       " 667: b'kya',\n",
       " 668: b'ck',\n",
       " 669: b'ter ',\n",
       " 670: b'wal',\n",
       " 671: b'ine ',\n",
       " 672: b'end ',\n",
       " 673: b'iket ',\n",
       " 674: b'aniket',\n",
       " 675: b'one',\n",
       " 676: b'der ',\n",
       " 677: b'saa',\n",
       " 678: b'cre',\n",
       " 679: b'mar',\n",
       " 680: b'us ',\n",
       " 681: b'Re',\n",
       " 682: b'hat ',\n",
       " 683: b'ee',\n",
       " 684: b'pass ',\n",
       " 685: b'bat',\n",
       " 686: b'du',\n",
       " 687: b'You ',\n",
       " 688: b'No ',\n",
       " 689: b') ',\n",
       " 690: b'rec',\n",
       " 691: b'por',\n",
       " 692: b'Yes',\n",
       " 693: b'gya ',\n",
       " 694: b'thi',\n",
       " 695: b'sk',\n",
       " 696: b'bol ',\n",
       " 697: b'ting ',\n",
       " 698: b'00 ',\n",
       " 699: b'kh ',\n",
       " 700: b'waala ',\n",
       " 701: b'mon',\n",
       " 702: b'the',\n",
       " 703: b'dis',\n",
       " 704: b'gr',\n",
       " 705: b'Hi ',\n",
       " 706: b'Will ',\n",
       " 707: b'ni',\n",
       " 708: b'Co',\n",
       " 709: b'ere',\n",
       " 710: b'office ',\n",
       " 711: b'kheal',\n",
       " 712: b'kha',\n",
       " 713: b'To ',\n",
       " 714: b'bhe',\n",
       " 715: b'mein ',\n",
       " 716: b'c ',\n",
       " 717: b'cus',\n",
       " 718: b'all',\n",
       " 719: b'Are ',\n",
       " 720: b'id ',\n",
       " 721: b'. I',\n",
       " 722: b'daa',\n",
       " 723: b'\\xf0\\x9f\\xa4',\n",
       " 724: b'I am ',\n",
       " 725: b'mode',\n",
       " 726: b'. B',\n",
       " 727: b'free ',\n",
       " 728: b'exp',\n",
       " 729: b'baat ',\n",
       " 730: b'thik ',\n",
       " 731: b'mail ',\n",
       " 732: b'ers ',\n",
       " 733: b'ant ',\n",
       " 734: b'fin',\n",
       " 735: b'? Yes ',\n",
       " 736: b'morro',\n",
       " 737: b'hua ',\n",
       " 738: b'hir ',\n",
       " 739: b'. An',\n",
       " 740: b'krna ',\n",
       " 741: b'Aa',\n",
       " 742: b'aam ',\n",
       " 743: b'dd',\n",
       " 744: b'ban',\n",
       " 745: b'ig',\n",
       " 746: b'raha h ',\n",
       " 747: b'ot ',\n",
       " 748: b'sh',\n",
       " 749: b'end',\n",
       " 750: b'\\xf0\\x9f\\x98\\x82 ',\n",
       " 751: b'lu',\n",
       " 752: b'check ',\n",
       " 753: b'ai ',\n",
       " 754: b'ound ',\n",
       " 755: b'ext ',\n",
       " 756: b'umba',\n",
       " 757: b'by ',\n",
       " 758: b'as we',\n",
       " 759: b'sagar',\n",
       " 760: b'yth',\n",
       " 761: b'har ',\n",
       " 762: b'messa',\n",
       " 763: b'tomorro',\n",
       " 764: b'fa',\n",
       " 765: b'Ch',\n",
       " 766: b'tk ',\n",
       " 767: b'6 ',\n",
       " 768: b'hot ',\n",
       " 769: b'Ok ',\n",
       " 770: b'ris',\n",
       " 771: b'hat',\n",
       " 772: b'toh ',\n",
       " 773: b'hi hai ',\n",
       " 774: b'enge ',\n",
       " 775: b'bol',\n",
       " 776: b'tom',\n",
       " 777: b'att',\n",
       " 778: b'ab ',\n",
       " 779: b'our ',\n",
       " 780: b'30 ',\n",
       " 781: b'der',\n",
       " 782: b'Hey ',\n",
       " 783: b'. Hao ',\n",
       " 784: b'Kya ',\n",
       " 785: b'ning ',\n",
       " 786: b'up ',\n",
       " 787: b'Ap',\n",
       " 788: b'jaa ',\n",
       " 789: b'conn',\n",
       " 790: b'shall ',\n",
       " 791: b'aap',\n",
       " 792: b'mber ',\n",
       " 793: b'Ni ',\n",
       " 794: b'ity ',\n",
       " 795: b'com',\n",
       " 796: b'chan',\n",
       " 797: b'iti',\n",
       " 798: b'This ',\n",
       " 799: b'cor',\n",
       " 800: b'there ',\n",
       " 801: b'goo',\n",
       " 802: b'hn',\n",
       " 803: b'khealte h ',\n",
       " 804: b'bata ',\n",
       " 805: b's, ',\n",
       " 806: b'ew ',\n",
       " 807: b'ed to ',\n",
       " 808: b'our',\n",
       " 809: b'they ',\n",
       " 810: b'card ',\n",
       " 811: b'Ek ',\n",
       " 812: b'ment',\n",
       " 813: b'dow',\n",
       " 814: b'M ',\n",
       " 815: b'ai',\n",
       " 816: b'jaayega ',\n",
       " 817: b'4 ',\n",
       " 818: b'baad ',\n",
       " 819: b'file ',\n",
       " 820: b'hel',\n",
       " 821: b'work ',\n",
       " 822: b'. Ok ',\n",
       " 823: b'9 ',\n",
       " 824: b'Goo',\n",
       " 825: b'pata ',\n",
       " 826: b'Pro',\n",
       " 827: b'tu',\n",
       " 828: b'dekh',\n",
       " 829: b'aha ',\n",
       " 830: b'10 ',\n",
       " 831: b'ence ',\n",
       " 832: b'know ',\n",
       " 833: b'dena ',\n",
       " 834: b'un ',\n",
       " 835: b'hiye ',\n",
       " 836: b'te hai ',\n",
       " 837: b'some ',\n",
       " 838: b'7 ',\n",
       " 839: b'bhj ',\n",
       " 840: b'lega',\n",
       " 841: b'Ha ',\n",
       " 842: b'message ',\n",
       " 843: b'here ',\n",
       " 844: b'appro',\n",
       " 845: b'20',\n",
       " 846: b'erko ',\n",
       " 847: b'shi',\n",
       " 848: b'ver ',\n",
       " 849: b'10',\n",
       " 850: b'line ',\n",
       " 851: b'. C',\n",
       " 852: b'lena ',\n",
       " 853: b'ding ',\n",
       " 854: b'??',\n",
       " 855: b'sion ',\n",
       " 856: b'ble ',\n",
       " 857: b'Ex',\n",
       " 858: b'str',\n",
       " 859: b'just ',\n",
       " 860: b'gu',\n",
       " 861: b'% ',\n",
       " 862: b'number ',\n",
       " 863: b'ir ',\n",
       " 864: b'Main ',\n",
       " 865: b'T ',\n",
       " 866: b'baar ',\n",
       " 867: b'ght',\n",
       " 868: b'red ',\n",
       " 869: b'y, ',\n",
       " 870: b'\\xf0\\x9f\\xa4\\xa3',\n",
       " 871: b'wala ',\n",
       " 872: b'nhi ',\n",
       " 873: b'. D',\n",
       " 874: b'dega ',\n",
       " 875: b'intern',\n",
       " 876: b'bi',\n",
       " 877: b'Is',\n",
       " 878: b'team ',\n",
       " 879: b'Gre',\n",
       " 880: b'ca',\n",
       " 881: b'now',\n",
       " 882: b'k liye ',\n",
       " 883: b'hai? ',\n",
       " 884: b'out',\n",
       " 885: b'some',\n",
       " 886: b'lic',\n",
       " 887: b'8 ',\n",
       " 888: b'ss',\n",
       " 889: b'. Thi',\n",
       " 890: b'pla',\n",
       " 891: b'av',\n",
       " 892: b'Mai ',\n",
       " 893: b'pp',\n",
       " 894: b'ane ',\n",
       " 895: b'Thik hai ',\n",
       " 896: b'hota ',\n",
       " 897: b'ut ',\n",
       " 898: b'lat',\n",
       " 899: b'\\xe2\\x80\\x99s ',\n",
       " 900: b'would ',\n",
       " 901: b'Sumit ',\n",
       " 902: b'A ',\n",
       " 903: b'AI ',\n",
       " 904: b'company ',\n",
       " 905: b'kha ',\n",
       " 906: b'kit',\n",
       " 907: b'And ',\n",
       " 908: b'gen',\n",
       " 909: b'tec',\n",
       " 910: b'hai.. ',\n",
       " 911: b'about ',\n",
       " 912: b'peri',\n",
       " 913: b'as well',\n",
       " 914: b'bhj',\n",
       " 915: b'pre',\n",
       " 916: b'if',\n",
       " 917: b'karna ',\n",
       " 918: b'main',\n",
       " 919: b'au',\n",
       " 920: b'kr k ',\n",
       " 921: b'ry ',\n",
       " 922: b'ea',\n",
       " 923: b'that',\n",
       " 924: b'him ',\n",
       " 925: b'. I ',\n",
       " 926: b'15',\n",
       " 927: b'gre',\n",
       " 928: b'liya ',\n",
       " 929: b'\\xf0\\x9f\\x91',\n",
       " 930: b'Mon',\n",
       " 931: b'. R',\n",
       " 932: b'don',\n",
       " 933: b'ven ',\n",
       " 934: b'load ',\n",
       " 935: b'wo',\n",
       " 936: b'ani ',\n",
       " 937: b'Than',\n",
       " 938: b'leave ',\n",
       " 939: b'ount ',\n",
       " 940: b'Abhi ',\n",
       " 941: b'sed ',\n",
       " 942: b'cont',\n",
       " 943: b'bola ',\n",
       " 944: b'ble',\n",
       " 945: b'eting ',\n",
       " 946: b'try ',\n",
       " 947: b'Yes ',\n",
       " 948: b'ving ',\n",
       " 949: b'hey ',\n",
       " 950: b'ans',\n",
       " 951: b'lease ',\n",
       " 952: b'when ',\n",
       " 953: b'gg',\n",
       " 954: b'k! ',\n",
       " 955: b'chl ',\n",
       " 956: b'ke liye ',\n",
       " 957: b'ok ',\n",
       " 958: b'ache',\n",
       " 959: b'star',\n",
       " 960: b'koi ',\n",
       " 961: b'hle ',\n",
       " 962: b'low ',\n",
       " 963: b'more ',\n",
       " 964: b'it, ',\n",
       " 965: b'hnan ',\n",
       " 966: b'tere ',\n",
       " 967: b'. L',\n",
       " 968: b'bhej ',\n",
       " 969: b'lenge ',\n",
       " 970: b'site ',\n",
       " 971: b'je ',\n",
       " 972: b'ance ',\n",
       " 973: b'ut',\n",
       " 974: b'ref',\n",
       " 975: b'dat',\n",
       " 976: b'able ',\n",
       " 977: b'ess',\n",
       " 978: b'pon',\n",
       " 979: b'mat',\n",
       " 980: b\"I'll \",\n",
       " 981: b'nahi ',\n",
       " 982: b'ta h ',\n",
       " 983: b'al khealte h ',\n",
       " 984: b'onal ',\n",
       " 985: b'fir',\n",
       " 986: b'Wo ',\n",
       " 987: b'whi',\n",
       " 988: b'plo',\n",
       " 989: b'web',\n",
       " 990: b'rha ',\n",
       " 991: b'rahe ',\n",
       " 992: b'Bo',\n",
       " 993: b'connect ',\n",
       " 994: b'ic ',\n",
       " 995: b'same ',\n",
       " 996: b'resu',\n",
       " 997: b'bana ',\n",
       " 998: b'tions ',\n",
       " 999: b'30',\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3e11ef-dc7f-45f9-8690-69e69033f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer):\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09561a44-ee6a-4394-9ab1-4137e16f17ed",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "Step:1 Word & Position embedding\n",
    "\n",
    "In this step,we convert a text into a list of tokens. Each token has an ID from the vocabulary. The shape of the tensor is 1x6 because we have one sentence with 6 tokens.\n",
    "\n",
    "Next, we use these tokens to find the corresponding embedding vector for each token. The vocab size is 1024,  so each token uses its ID to look up the right vector in the token embedding table. We do the same for positional embeddings, which have 256 rows because the block size is 256. This means the model can only handle sequences with up to 256 tokens.\n",
    "\n",
    "After getting the token and positional embeddings, we add them together. This results in a tensor of size 1x6x768, where 1 is the number of inputs, 6 is the number of tokens, and 768 is the size of the embedding vectors. This output is then sent to the block layer.\n",
    "\n",
    "![Transformer Step 1](https://raw.githubusercontent.com/ImadSaddik/Train_Your_Language_Model_Course/e9e8e01b46e1376406bd1c3a0e1692b64ba660ea/images/transformer_step_1.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01024a0c-940b-42cb-bf02-63b8bc8d5a84",
   "metadata": {},
   "source": [
    "## Step 2: Multi-Head Attention\n",
    "\n",
    "We take the tensor from the previous step and pass it to the multi-head attention layer. This layer has two settings: `head size and number of heads`. These settings split the attention block into smaller parts called heads. All heads process the input at the same time to speed up calculations.\n",
    "\n",
    "The goal of multi-head attention is to help the model focus on different parts of the input at once. Each head can learn to look at different relationships between words or tokens. Since they work in parallel, the model can understand patterns in the data more effectively.\n",
    "\n",
    "Each head produces a tensor of size 1x6x128, where 6 is the number of heads and 128 is the size of each head. We then combine all the outputs into a 1x6x768 tensor. Finally, this is passed through a feed-forward layer, which adjusts the last dimension to 768, matching the embedding size.\n",
    "\n",
    "We can stack multiple multi-head attention blocks to deepen the model's understanding of the input. This allows it to learn more complex patterns and relationships. In the image, we have stacked four layers to enhance its ability to process the data.\n",
    "\n",
    "![Transformer Step 2](https://raw.githubusercontent.com/ImadSaddik/Train_Your_Language_Model_Course/e9e8e01b46e1376406bd1c3a0e1692b64ba660ea/images/transformer_step_2.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742d5429-b1ca-4434-939c-a3bcf3d97225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc719d69-a5af-440b-8753-f66ae7b5ee4c",
   "metadata": {},
   "source": [
    "### 1. Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5863eaef-f828-46be-b078-36298714ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Each word gets turned into:\n",
    "\n",
    "# a query (what it's looking for)\n",
    "\n",
    "# a key (how it can be found)\n",
    "\n",
    "# a value (what information it has)\n",
    "\n",
    "# These are all made with nn.Linear, which means we’re learning how to turn input embeddings into these things.\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "    def __init__(self,head_size:int):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril',torch.tril(\n",
    "            torch.ones(block_size,block_size)\n",
    "        )) #We’re making a triangle mask ⬛⬛⬜⬜... that ensures a word can only look at earlier words, not future ones (important for things like language generation).\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        _, T, _ = x.shape\n",
    "        k = self.key(x)   # (B,T,hs) # turn Each word (input) into keys\n",
    "        q = self.query(x)  # (B,T,hs) # turn Each word (input) into queries\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  #We’re seeing how well each word's query matches every word’s key using a dot product (matrix multiplication). This gives us a score matrix of shape (B, T, T) — how much attention each word gives to every other word.\n",
    "\n",
    "# We also scale it by the size of the key to keep the numbers stable.\n",
    "        weights = weights.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T) We apply the triangle mask here. So future words get -inf as their score — meaning they won’t be attended to.\n",
    "        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = weights @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs) Now each word gathers information from the values of other words, based on the attention weights.\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004312f4-1b0f-42bf-ae5d-08ba646dac52",
   "metadata": {},
   "source": [
    "### 2. MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af42a1f7-7e46-4f2f-987f-363172ea180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention running in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a list of independent attention heads.\n",
    "        # Each head will learn to focus on different parts of the input.\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "        # A linear layer to combine all the outputs from the different heads\n",
    "        # into a single vector for each position in the input.\n",
    "        self.projection = nn.Linear(head_size * num_heads, n_embd)\n",
    "\n",
    "        # A dropout layer to randomly drop some connections during training,\n",
    "        # helping the model generalize better and not overfit.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Run input x through all attention heads.\n",
    "        # Each head returns a tensor of shape [batch, sequence_len, head_size].\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # Concatenate outputs from all heads along the last dimension\n",
    "        # Resulting shape: [batch, sequence_len, head_size * num_heads]\n",
    "\n",
    "        # Project the concatenated output to match the original embedding size (n_embd)\n",
    "        # Then apply dropout to the result\n",
    "        out = self.dropout(self.projection(out))\n",
    "\n",
    "        # Return the final processed tensor\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d188b5d-cc6c-4f61-8604-9b1065d9d99b",
   "metadata": {},
   "source": [
    "### 3. Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4133a09-444b-44de-a26e-f1a99921f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # First linear layer expands the dimension\n",
    "            nn.ReLU(),                     # Adds non-linearity so model can learn complex patterns\n",
    "            nn.Linear(4 * n_embd, n_embd), # Brings it back to original embedding size\n",
    "            nn.Dropout(dropout),           # Randomly drops values to prevent overfitting\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)  # Simply run input through the feed-forward network\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int) -> None:\n",
    "        # n_embd: embedding size (width of data), n_head: how many attention heads\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embd // n_head  # Each head looks at part of the embedding\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)  # Multiple attention heads\n",
    "        self.feed_forward = FeedForward(n_embd)                      # Simple feed-forward network\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)  # Normalizes input for stability\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)  # Same, but for feed-forward part\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # First do attention, with layer norm and residual connection\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "\n",
    "        # Then do feed-forward, again with layer norm and residual connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740b962-4ebd-4e94-aaa7-1fdf90a11795",
   "metadata": {},
   "source": [
    "### 4. Assembling the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e58dd4-30a0-41c1-ba41-2c2c53614bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Lookup table to get the embedding vector for each token in the vocabulary\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Lookup table to get the embedding for each position in the sequence\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # A stack of transformer blocks (attention + feedforward layers)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        # Normalize the output from the transformer blocks\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Final linear layer maps the output embeddings to vocab size logits\n",
    "        self.final_linear_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # Apply custom weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        # Initializes weights with normal distribution (mean=0, std=0.02)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, input_tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_tokens: Tensor of token indices of shape (batch_size, sequence_length)\n",
    "            targets: Optional tensor of target token indices of same shape as input_tokens\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (logits, loss) where logits has shape (batch_size, sequence_length, vocab_size)\n",
    "            and loss is optional cross-entropy loss if targets are provided\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = input_tokens.shape  # Batch size, Sequence length\n",
    "\n",
    "        # Get token embeddings: shape (B, T, C)\n",
    "        token_embedding = self.token_embedding_table(input_tokens)\n",
    "\n",
    "        # Get positional embeddings for each position (0 to T-1)\n",
    "        positional_embedding = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "\n",
    "        # Add token + position embeddings to mix both meanings\n",
    "        x = token_embedding + positional_embedding  # (B, T, C)\n",
    "\n",
    "        # Pass through the stack of transformer blocks\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "\n",
    "        # Normalize the output before final prediction\n",
    "        x = self.final_layer_norm(x)  # (B, T, C)\n",
    "\n",
    "        # Map to vocabulary logits (i.e., unnormalized probabilities)\n",
    "        logits = self.final_linear_layer(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # If we have target tokens, calculate the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Flatten logits and targets for cross-entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "                Generate new tokens given a context.\n",
    "\n",
    "                Args:>ns: Starting token indices of shape (batch_size, sequence_length)\n",
    "                        max_new_tokens: Number of new tokens to generate\n",
    "\n",
    "                Returns:\n",
    "                        Tensor of token indices of shape (batch_size, sequence_length + max_new_tokens)\n",
    "                \"\"\"\n",
    "\n",
    "        # input_tokens is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop input_tokens to the last block_size tokens\n",
    "            cropped_input = input_tokens[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(cropped_input)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            input_tokens = torch.cat(\n",
    "                (input_tokens, idx_next), dim=1)  # (B, T+1)\n",
    "        return input_tokens\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1db87-daed-4cde-93cc-d76b0eda360c",
   "metadata": {},
   "source": [
    "### 5. Parameters & dummy input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146d0f0d-7042-498d-8598-93465fd818fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.53409 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b522a8-b5a7-4d53-a1bb-cfd839ee2335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 1034]) None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_length = 6\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "x = x.to(device)\n",
    "\n",
    "logits, loss = model(x)\n",
    "print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c566307b-80b9-4e4b-b3be-c4627978e5d2",
   "metadata": {},
   "source": [
    "### Display the model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b71e8e-e41e-43bb-9419-cd7a904db0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├─ token_embedding_table: Embedding (397,056 parameters)\n",
      "├─ position_embedding_table: Embedding (98,304 parameters)\n",
      "├─ blocks: Sequential (10,639,872 parameters)\n",
      "│  ├─ 0: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 1: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 2: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 3: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 4: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 5: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "├─ final_layer_norm: LayerNorm (768 parameters)\n",
      "├─ final_linear_layer: Linear (398,090 parameters)\n"
     ]
    }
   ],
   "source": [
    "def print_model_structure(model: torch.nn.Module, indent: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Custom function to print model structure in a hierarchical format\n",
    "    \"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        params = sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{indent}├─ {name}: {child.__class__.__name__} ({params:,} parameters)\")\n",
    "        print_model_structure(child, indent + '│  ')\n",
    "\n",
    "\n",
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5ca45-d3b0-4cf3-8679-b4d683f270e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add .\n",
    "! git commit -m \"added transformer layer\"\n",
    "! git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b2c83-175a-49da-96de-05d88ef5ff91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec3103-e61e-40d9-95ff-4d7e477ecc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69c85b-bbeb-4e86-865c-7a6bca8bf3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a6919-4e1d-4165-8052-c175e70d005e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2954e3-1457-4553-913e-f1100bfc782e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d40ec-d4af-4505-bffd-d28679aa5923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b46635-7eb8-4388-9886-43d8e6e76c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f0f61c-0a84-4931-bb9b-80701997f1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2fcd6-6df0-4142-a866-d5184889b684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f5f25-036f-402f-8715-7da698b094bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38c9d6-5b40-40a6-a37b-27f82c375520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
